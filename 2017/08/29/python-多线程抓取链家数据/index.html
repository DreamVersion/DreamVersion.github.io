<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> python 多线程抓取链家数据 · MelonQi Blog</title><meta name="description" content="python 多线程抓取链家数据 - MelonQi"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://melonqi.cn/atom.xml" title="MelonQi Blog"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="http://weibo.com/u/3416592410" target="_blank" class="nav-list-link">WEIBO</a></li><li class="nav-list-item"><a href="https://github.com/DreamVersion" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">python 多线程抓取链家数据</h1><div class="post-info">Aug 29, 2017</div><div class="post-content"><p>Python在程序并行化方面多少有些声名狼藉, 基于<a href="https://segmentfault.com/a/1190000000414339" target="_blank" rel="external">一行 Python 实现并行化</a>, 可以使用multiprocessing库中的map实现并兴化操作,简单快捷.</p>
<a id="more"></a>
<h2 id="map_u5B9E_u73B0_u5E76_u884C_u5316"><a href="#map_u5B9E_u73B0_u5E76_u884C_u5316" class="headerlink" title="map实现并行化"></a>map实现并行化</h2><p>map函数会根据提供的函数对指定序列做映射, 常用的类似函数有reduce,filter. 这里使用multiprocessing 和它鲜为人知的子库 multiprocessing.dummy中的map函数,可以帮助我们很好地实现并行化.</p>
<p>dummy 是 multiprocessing 模块的完整克隆,唯一的不同在于 multiprocessing 作用于进程,而 dummy 模块作用于线程(因此也包括了 Python 所有常见的多线程限制).所以替换使用这两个库异常容易. 你可以针对 IO 密集型任务和 CPU 密集型任务来选择不同的库.简言之, IO 密集型任务选择multiprocessing.dummy, CPU 密集型任务选择multiprocessing.</p>
<p>举例:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib2 </span><br><span class="line"><span class="keyword">from</span> multiprocessing.dummy <span class="keyword">import</span> Pool <span class="keyword">as</span> ThreadPool </span><br><span class="line"></span><br><span class="line">urls = [</span><br><span class="line">    <span class="string">'http://www.python.org'</span>, </span><br><span class="line">    <span class="string">'http://www.python.org/about/'</span>,</span><br><span class="line">    <span class="string">'http://www.onlamp.com/pub/a/python/2003/04/17/metaclasses.html'</span>,</span><br><span class="line">    <span class="string">'http://www.python.org/doc/'</span>,</span><br><span class="line">    <span class="string">'http://www.python.org/download/'</span>,</span><br><span class="line">    <span class="string">'http://www.python.org/getit/'</span>,</span><br><span class="line">    <span class="string">'http://www.python.org/community/'</span>,</span><br><span class="line">    <span class="string">'https://wiki.python.org/moin/'</span>,</span><br><span class="line">    <span class="string">'http://planet.python.org/'</span>,</span><br><span class="line">    <span class="string">'https://wiki.python.org/moin/LocalUserGroups'</span>,</span><br><span class="line">    <span class="string">'http://www.python.org/psf/'</span>,</span><br><span class="line">    <span class="string">'http://docs.python.org/devguide/'</span>,</span><br><span class="line">    <span class="string">'http://www.python.org/community/awards/'</span></span><br><span class="line">    <span class="comment"># etc.. </span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make the Pool of workers</span></span><br><span class="line">pool = ThreadPool(<span class="number">4</span>) </span><br><span class="line"><span class="comment"># Open the urls in their own threads</span></span><br><span class="line"><span class="comment"># and return the results</span></span><br><span class="line">results = pool.map(urllib2.urlopen, urls)</span><br><span class="line"><span class="comment">#close the pool and wait for the work to finish </span></span><br><span class="line">pool.close() </span><br><span class="line">pool.join()</span><br></pre></td></tr></table></figure>
<p>关于性能测试,原文有简单的demo测试,可以查看原文<a href="https://segmentfault.com/a/1190000000414339" target="_blank" rel="external">一行 Python 实现并行化</a>. </p>
<h2 id="u6293_u53D6_u5317_u4EAC_u5E02_u4E8C_u624B_u623F_u6570_u636E"><a href="#u6293_u53D6_u5317_u4EAC_u5E02_u4E8C_u624B_u623F_u6570_u636E" class="headerlink" title="抓取北京市二手房数据"></a>抓取北京市二手房数据</h2><p>结合bs4,map我们可以很好地抓取全北京市地二手房数据；北京市的二手房划分为不同的区,针对某个区进行抓取数据就可以了. </p>
<p>关于bs4的使用可以参考<a href="http://dreamversion.github.io/2017/08/20/BeatifulSoup-regex-%E8%A7%A3%E6%9E%90%E9%93%BE%E5%AE%B6%E6%88%BF%E6%BA%90%E4%BF%A1%E6%81%AF/" target="_blank" rel="external">BeautifulSoup &amp; regex 解析链家房源信息</a></p>
<p>完整的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> multiprocessing.dummy <span class="keyword">import</span> Pool <span class="keyword">as</span> ThreadPool</span><br><span class="line"><span class="keyword">import</span> bs4</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> demjson</span><br><span class="line"></span><br><span class="line"><span class="comment"># parse single house</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_house</span><span class="params">(house_id)</span>:</span></span><br><span class="line">    house_url = <span class="string">"http://bj.lianjia.com/ershoufang/"</span> + house_id + <span class="string">".html"</span></span><br><span class="line">    <span class="keyword">print</span> house_url</span><br><span class="line">    r = requests.get(house_url)</span><br><span class="line">    soup = bs4.BeautifulSoup(r.content, <span class="string">"lxml"</span>)</span><br><span class="line">    pattern = re.compile(<span class="string">r'init\((.*?)\);'</span>, re.DOTALL)</span><br><span class="line">    <span class="keyword">for</span> script <span class="keyword">in</span> soup.find_all(<span class="string">'script'</span>):</span><br><span class="line">        <span class="keyword">if</span> type(script.string) == bs4.element.NavigableString:</span><br><span class="line">            <span class="keyword">if</span> script.text.find(<span class="string">"sellDetail"</span>) &gt;= <span class="number">0</span>:</span><br><span class="line">                match = pattern.search(script.text)</span><br><span class="line">                <span class="keyword">if</span> match:</span><br><span class="line">                    json_dict = demjson.decode(match.group(<span class="number">1</span>))</span><br><span class="line">                    house_info = json.dumps(json_dict, indent=<span class="number">4</span>)</span><br><span class="line">                    <span class="keyword">print</span> house_info</span><br><span class="line"></span><br><span class="line"><span class="comment"># get house ids</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_house_page</span><span class="params">(url)</span>:</span></span><br><span class="line">    r = requests.get(url)</span><br><span class="line">    soup = bs4.BeautifulSoup(r.content, <span class="string">'lxml'</span>)</span><br><span class="line">    pattern = re.compile(<span class="string">r'main\((.*?)\);'</span>, re.DOTALL)</span><br><span class="line">    <span class="keyword">for</span> script <span class="keyword">in</span> soup.find_all(<span class="string">'script'</span>):</span><br><span class="line">        <span class="keyword">if</span> type(script.string) == bs4.element.NavigableString:</span><br><span class="line">            <span class="keyword">if</span> script.text.find(<span class="string">"sellList"</span>) &gt;= <span class="number">0</span>:</span><br><span class="line">                match = pattern.search(script.text)</span><br><span class="line">                <span class="keyword">if</span> match:</span><br><span class="line">                    json_dict = demjson.decode(match.group(<span class="number">1</span>))</span><br><span class="line">                    house_ids = json_dict[<span class="string">'ids'</span>].split(<span class="string">','</span>)</span><br><span class="line">                    <span class="keyword">for</span> house_id <span class="keyword">in</span> house_ids:</span><br><span class="line">                        parse_house(house_id)</span><br><span class="line"></span><br><span class="line"><span class="comment"># parse a district house</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_district</span><span class="params">(url)</span>:</span></span><br><span class="line">    r = requests.get(url)</span><br><span class="line">    <span class="keyword">print</span> r.content</span><br><span class="line">    soup = bs4.BeautifulSoup(r.content, <span class="string">'lxml'</span>)</span><br><span class="line">    total_page = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> result <span class="keyword">in</span> soup.find_all(<span class="string">'div'</span>, class_=<span class="string">'page-box fr'</span>):</span><br><span class="line">        page_box = result.contents[<span class="number">0</span>]</span><br><span class="line">        total_page_json = json.loads(page_box.attrs[<span class="string">'page-data'</span>])</span><br><span class="line">        total_page = total_page_json[<span class="string">'totalPage'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>, total_page + <span class="number">1</span>):</span><br><span class="line">        page_url = url + <span class="string">"/pg"</span> + str(page) + <span class="string">"/"</span></span><br><span class="line">        <span class="keyword">print</span> page_url</span><br><span class="line">        parse_house_page(page_url)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scapy_district</span><span class="params">(district_name)</span>:</span></span><br><span class="line">    url = <span class="string">"http://bj.lianjia.com/ershoufang/"</span> + district_name + <span class="string">"/"</span></span><br><span class="line">    <span class="keyword">print</span> url</span><br><span class="line">    parse_district(url)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    districts = [<span class="string">'dongcheng'</span>, <span class="string">'xicheng'</span>, <span class="string">'chaoyang'</span>, <span class="string">'haidian'</span>, <span class="string">'fengtai'</span>, <span class="string">'shijingshan'</span>, <span class="string">'tongzhou'</span>, <span class="string">'changping'</span>,</span><br><span class="line">                 <span class="string">'daxing'</span>, <span class="string">'yizhuangkaifaqu'</span>, <span class="string">'shunyi'</span>, <span class="string">'fangshan'</span>, <span class="string">'mentougou'</span>, <span class="string">'pinggu'</span>, <span class="string">'huairou'</span>, <span class="string">'miyun'</span>,</span><br><span class="line">                 <span class="string">'yanqing'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># districts = ['dongcheng']</span></span><br><span class="line"></span><br><span class="line">    pool = ThreadPool(<span class="number">4</span>)</span><br><span class="line">    results = pool.map(scapy_district, districts)</span><br><span class="line">    <span class="comment"># close the pool and wait for the work to finish</span></span><br><span class="line">    pool.close()</span><br><span class="line">    pool.join()</span><br></pre></td></tr></table></figure>
<h2 id="u95EE_u9898"><a href="#u95EE_u9898" class="headerlink" title="问题"></a>问题</h2><p>在爬取的过程中会遇到链家防爬虫的阻碍,连一个区的二手房都抓取不完就不能再继续抓取了. 简单分析了一下, 链家会根据ip+有验证功能的时效性cookie来判断是不是恶意爬虫. 一旦发现了恶意流量,会返回一组图片,让人根据提示选择对应的图片,以此来判断是否是机器人. 如果选择正确会返回一个有效cookie,后续拿这个有效cookie就可以继续访问. </p>
<p>所以破解思路：</p>
<ol>
<li><p>使用代理来变换ip,由于正常的爬虫代理比较贵,只好寻求一些免费的代理,比如<a href="http://www.xicidaili.com/" target="_blank" rel="external">西刺</a>,有很多免费代理,但是大部分不能用,需要自己实现对网址解析,获取有效代理形成代理池,每次获取链家数据就使用代理池的代理进行访问；难点是实现自己的代理池,而且有效代理的数量和质量无法保证. </p>
</li>
<li><p>识别出图片,或者机器随机选择图片,瞎猫碰着死耗子的方法来获取有效地cookie</p>
</li>
<li><p>隔一段时间等cookie自动失效再做操作,缺点是保证不了数据的实时性和正确性</p>
</li>
</ol>
<p>目前可行的方案是方案1,下一步实现一个属于自己的免费代理池,Fighting!</p>
</div></article></div></main><footer><div class="paginator"><a href="/2017/09/21/AgentPool/" class="prev">上一篇</a><a href="/2017/08/20/BeatifulSoup-regex-解析链家房源信息/" class="next">下一篇</a></div><div data-thread-key="2017/08/29/python-多线程抓取链家数据/" data-title="python 多线程抓取链家数据" data-url="http://melonqi.cn/2017/08/29/python-多线程抓取链家数据/" data-author-key="1" class="ds-thread"></div><script>var duoshuoQuery = {short_name:"melonqi"};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
})();

</script><div class="copyright"><p>© 2015 - 2017 <a href="http://melonqi.cn">MelonQi</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-65933410-1",'auto');ga('send','pageview');</script></body></html>